<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="3D Rotation of Radiographs with Diffusion Models.">
  <meta name="keywords" content="Diffusion Models, Radiograph, Generative AI, Radiology, 3D">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RadRotator: 3D Rotation of Radiographs with Diffusion Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">RadRotator: <br />3D Rotation of Radiographs with Diffusion Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://x.com/PRouzrokh">Pouria Rouzrokh, MD MPH MHPE</a><sup>1,2,§</sup>,</span>
            <span class="author-block">
              <a href="https://x.com/Khosravi_Bardia">Bardia Khosravi, MD MPH MHPE</a><sup>1,2,§</sup>,</span>
            <span class="author-block">
              <a href="https://x.com/ShahriarFaghani">Shahriar Faghani, MD</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Kellen L. Mulford, PhD</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://x.com/tauntonm">Michael J. Taunton, MD</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://x.com/Slowvak">Bradley J. Erickson, MD PhD</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://x.com/CodyWylesMD">Cody C. Wyles, MD</a><sup>2,*</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <br />
            <span class="author-block"><sup>1</sup> Department of Radiology, Mayo Clinic, Rochester, MN, USA</span>
            <span class="author-block"><sup>2</sup> Department of Orthopedic Surgery, Mayo Clinic, Rochester, MN, USA</span>
            <br><br>
            <span class="author-block"><sup>§</sup> Equal contribution</span>
            <span class="author-block"><sup>*</sup> Corresponding Author <a href="mailto:wyles.cody@mayo.edu" style="text-decoration:None; color:#4a4a4a !important"><u>(Email)</u></a></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/Ru3pM8rcqWI"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Demo Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/spaces/Pouriarouzrokh/RadRotator"
                    class="external-link button is-normal is-rounded is-dark" target="_blank">
                  <span class="icon">
                      <i class="fas fa-rocket"></i>
                  </span>
                  <span>Demo</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/xr_2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/xr_1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/xr_3.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/xr_4.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/xr_5.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/xr_6.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/xr_7.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/xr_8.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/xr_9.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Abstract. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Transforming two-dimensional (2D) images into three-dimensional (3D) volumes is a well-known, 
            yet challenging problem for the computer vision community. In the medical domain, 
            a few previous studies attempted to convert two or more input radiographs into computed tomography (CT) volumes. 
            Following their effort, we introduce a diffusion model-based technology that can rotate the anatomical 
            content of any input radiograph in 3D space, potentially enabling the visualization of 
            the entire anatomical content of the radiograph in 3D. Similar to previous studies, we relied on CT volumes 
            to acquire Digitally Reconstructed Radiographs (DRRs) as the training data for our model. However, 
            we addressed two significant limitations encountered in previous studies: 1. We utilized conditional 
            diffusion models with classifier-free guidance instead of Generative Adversarial Networks (GANs) to 
            achieve higher mode coverage and improved output image quality, with the only trade-off being the inference time, 
            which is often less critical in medical applications; and 2. We demonstrated that the unreliable use of 
            style transfer deep learning (DL) models, such as Cycle-GAN, to transfer the style of actual radiographs 
            to DRRs could be replaced with a simple, yet effective training transformation that randomly changes the 
            pixel intensity histograms of the input and ground-truth imaging data during training. This transformation 
            makes the diffusion model agnostic to any distribution variations of the input data pixel intensity, 
            enabling the reliable training of a DL model on input DRRs and applying the exact same model to conventional 
            radiographs (or DRRs) during inference.
          </p>
        </div>
        <img src="./static/images/model.png" border=1 alt="Abstract image." /> 
      </div>
    </div>
  </div>
</section>
<!--/ Abstract. -->

<!-- Highlighted Findings. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div>
        <h2 class="title is-3">Highlighted Findings</h2>
        <br>

        <!--Highlight 1-->
        <div class="content has-text-justified">
        <h3>1. CycleGAN is Insufficient for DRR to XR Style Transfer</h3>
          <p>
            Training a deep learning model to rotate 2D radiographs in 3D space encounters a significant limitation: the absence of actual radiograph pairs 
            with known transformations between them to serve as ground truth data for training. To address this, we adopted the approach of several prior studies that attempted building
            3D volumes from 2D radiographs: synthesizing "virtual" radiographs from computed tomography (CT) data. These virtual radiographs, known as Digitally Reconstructed Radiographs (DRRs), 
            offer a major advantage: being programmatically created, they allow us to precisely control the differences between two DRRs in 3D space. 
            Furthermore, we can theoretically generate an infinite number of radiographs from a single CT volume. This method should, in theory, 
            provide ample training data for a model designed to rotate radiographs. However, there exists a subtle yet crucial issue: the style (contrast and brightness) 
            of DRRs differs significantly from that of standard radiographs. Consequently, a model trained on DRRs is unlikely to generalize effectively to conventional radiographs.
          </p>
          <img src="./static/images/DRRs.png" border=1 alt="Abstract image." /> 
          <br>
          <br>
          <p>     
            The studies in question addressed this issue by employing CycleGAN (or its variants), an unpaired image-to-image style transfer deep learning model 
            designed to adapt the style of radiographs to DRRs without altering their content. Following this lead, we too attempted this strategy, training 
            a CycleGAN on over 200,000 pairs of DRRs and radiographs from our dataset. However, we encountered a critical realization: given the absence of 
            rotated radiographs in our training dataset, CycleGAN was unable to learn the process of applying the style of radiographs to rotated DRRs. 
            Consequently, it simultaneously altered the style of rotated DRRs and transformed their content to resemble that of non-rotated radiographs. 
            This represented a significant limitation in our research.
          </p>
          <img src="./static/images/CycleGAN.png" border=1 alt="Abstract image." />
        </div>
        <!--Highlight 1-->

        <!--Highlight 2-->
        <div class="content has-text-justified">
          <h3>2. All You Need for Style Transfer is RandHistogramShift!</h3>
          <p>
            As a result of the Cycle-GAN performance, we replaced it with the inclusion of the RandHistogramShift transformation in our 
            data preprocessing pipeline during the training phase. The logic behind the RandHistogramShift transfer is straightforward 
            and effective. We hypothesized that the primary distinction between a DRR and a radiograph lies in their pixel intensity distributions, 
            affecting their overall brightness and contrast. Therefore, by making the DDPM agnostic to the pixel intensity distribution of 
            the input images, it should, in theory, become capable of generalizing to any type of radiography data (e.g., X-rays, DRRs, or 
            fluoroscopy shots), despite being solely trained on DRRs. Our approach implementation is shown in Figure 5. During training, 
            we generated a random image filled with stochastic noise and then applied image processing techniques to align the histogram 
            of each pair of input and output DRRs with that of the noisy image. Given that the noisy image is randomly generated at each training step, 
            the DDPM is pushed to disregard the stochastic pixel intensity distributions of the ground truth DRR pairs, focusing instead on their 
            content to learn the 3D rotations.
          </p>
          <img src="./static/images/RandHistogramShift.png" border=1 alt="Abstract image." /> 
        </div>
        <!--Highlight 2-->

        <!--Highlight 3-->
        <div class="content has-text-justified">
        <h3>3. Consistent View Transformation Capabalities</h3>
          <p>
            In our methodology, we trained a conditional Denoising Diffusion Probabilistic Model (DDPM) that accepts an input radiograph 
            along with a set of transformations (along the x, y, and z axes) and outputs a radiograph rotated according to those specified transformations. 
            The transformation values used during training were chosen from a predefined set: [-15°, +15°]. 
            As anticipated, our model successfully learned to apply transformations corresponding to one of these values in each axis to the input radiograph:
          </p>
          <img src="./static/images/results_1.png" border=1 alt="Abstract image." /> 
          <br>
          <br>
          <img src="./static/images/results_2.png" border=1 alt="Abstract image." /> 
          <br>
          <br>
          <p>
            However, we also discovered that our model can apply consistent transformations to the input radiograph across the entire spectrum of [-15°, +15°] along any of the three axes. 
            The key for doing so lies in feeding a constant noisy image to the model during inference for all predictions along the desired spectrum.
            We leveraged this capability, combined with interpolation techniques in the embedding space, to generate a comprehensive range 
            of rotations for various input radiographs. This approach effectively simulates the consistent rotation of an anatomical 
            volume in 3D space, illustrating the model's versatility in creating detailed and varied transformations. 
            You can try this by playing with the slider below:
          </p>
          <!-- Interpolating. -->
        <div class="content has-text-justified">
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="330" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>
        <!--/ Interpolating. -->
          <p>
            Please look at our <a href="#" target="_blank"><b>paper</b></a> and <a href="" target="_blank"><b>demo</b></a> for more information.
          </p>
        </div> 
        <!--Highlight 3-->
      </div>
    </div>
  </div>
</section>
<!--/ Highlighted Findings. -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <div class="has-text-centered"><h2 class="title">BibTeX</h2></div>
    <pre><code>@article{Rouzrokh,
    title = {RadRotator: 3D Rotation of Radiographs with Diffusion Models},
    year = {2024},
    doi = {https://doi.org/10.1016/j.cmpb.2023.107832},
    url = {https://www.sciencedirect.com/science/article/pii/S0169260723004984},
    author = {Pouria Rouzrokh and Bardia Khosravi and Shahriar Faghani and Kellen Mulford and Michael J. Taunton and Bradley J. Erickson and Cody C. Wyles}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="#">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/PouriaRouzrokh/RadRotator" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content is-centered has-text-centered">
          <p>
            Thanks to <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> for open-sourcing this website code.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
